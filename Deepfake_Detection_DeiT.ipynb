{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msrishav-28/DeiT_model/blob/main/Deepfake_Detection_DeiT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WEK2k1yOVSB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "190f0d13-7820-4f43-b317-77232a5009a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Install required packages\\n!pip install timm==0.6.12\\n!pip install opencv-python==4.8.0.76\\n!pip install albumentations==1.3.1\\n!pip install facenet-pytorch==2.5.3\\n!pip install wandb==0.15.12\\n!pip install onnx==1.14.1\\n!pip install av==10.0.0\\n!pip install ffmpeg-python==0.2.0\\n!pip install scikit-learn==1.3.0\\n!pip install tqdm==4.66.1\\n\\n# Mount Google Drive\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Comprehensive DeiT Deepfake Detection Framework\n",
        "# For FaceForensics++ (c40) and Celeb-DF datasets\n",
        "\n",
        "# Cell 1: Installation and Setup\n",
        "\"\"\"\n",
        "# Install required packages\n",
        "!pip install timm==0.6.12\n",
        "!pip install opencv-python==4.8.0.76\n",
        "!pip install albumentations==1.3.1\n",
        "!pip install facenet-pytorch==2.5.3\n",
        "!pip install wandb==0.15.12\n",
        "!pip install onnx==1.14.1\n",
        "!pip install av==10.0.0\n",
        "!pip install ffmpeg-python==0.2.0\n",
        "!pip install scikit-learn==1.3.0\n",
        "!pip install tqdm==4.66.1\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install av"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUEtyTcfKAVr",
        "outputId": "6891cb79-bcdc-4440-a480-f9f68d4abac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting av\n",
            "  Downloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Downloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-14.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Import Libraries\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from timm import create_model\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from PIL import Image\n",
        "import av\n",
        "from facenet_pytorch import MTCNN\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "P4KZqvP4e9e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "89880e78-1a71-4811-c56e-235b00fb9f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'facenet_pytorch'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-78e9c440afd5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mav\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfacenet_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'facenet_pytorch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Set Random Seeds\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()"
      ],
      "metadata": {
        "id": "BF62owCLfDW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Project Configuration\n",
        "class Config:\n",
        "    # Project paths\n",
        "    base_path = \"/content/drive/MyDrive/deepfake_detection\"\n",
        "\n",
        "    # FaceForensics++ configuration\n",
        "    ff_path = f\"{base_path}/FaceForensics++\"\n",
        "    ff_compression = \"c40\"\n",
        "    ff_methods = [\"Deepfakes\", \"Face2Face\", \"FaceSwap\", \"NeuralTextures\"]\n",
        "    ff_samples_per_method = 500  # 500 samples from each category\n",
        "\n",
        "    # Celeb-DF configuration\n",
        "    celebdf_path = f\"{base_path}/Celeb-DF\"\n",
        "    celebdf_v2_path = f\"{base_path}/Celeb-DF-v2\"\n",
        "    celebdf_samples = 2000  # Samples to use from Celeb-DF\n",
        "\n",
        "    # Frame extraction\n",
        "    frames_per_video = 20\n",
        "    face_margin = 0.3  # Margin around detected face\n",
        "    min_face_size = 48  # Minimum face size to consider\n",
        "\n",
        "    # Dataset configuration\n",
        "    image_size = 224\n",
        "    batch_size = 32\n",
        "    num_workers = 2\n",
        "    train_ratio = 0.8\n",
        "    val_ratio = 0.1\n",
        "    test_ratio = 0.1\n",
        "\n",
        "    # Model configuration\n",
        "    model_name = \"deit_small_patch16_224\"\n",
        "    pretrained = True\n",
        "    num_classes = 2\n",
        "\n",
        "    # Training configuration\n",
        "    num_epochs = 20\n",
        "    learning_rate = 2e-5\n",
        "    weight_decay = 1e-4\n",
        "    warmup_epochs = 2\n",
        "    checkpoint_dir = f\"{base_path}/checkpoints\"\n",
        "    results_dir = f\"{base_path}/results\"\n",
        "\n",
        "    # Advanced training features\n",
        "    mixed_precision = True\n",
        "    gradient_accumulation_steps = 1\n",
        "    gradient_clip_val = 1.0\n",
        "    label_smoothing = 0.1\n",
        "    mixup_alpha = 0.2\n",
        "\n",
        "    # Save paths\n",
        "    save_frequency = 1\n",
        "\n",
        "    # Logging\n",
        "    use_wandb = False\n",
        "    project_name = \"deepfake-detection\"\n",
        "    experiment_name = \"deit-ffpp-celebdf\"\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Make sure directories exist\n",
        "os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
        "os.makedirs(config.results_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "EHbU5z6ygGIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Video Processing Utilities\n",
        "class VideoProcessor:\n",
        "    \"\"\"Video processing utilities for deepfake detection\"\"\"\n",
        "    def __init__(self, config, device='cuda'):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize face detector\n",
        "        self.face_detector = MTCNN(\n",
        "            keep_all=True,\n",
        "            post_process=False,\n",
        "            min_face_size=self.config.min_face_size,\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "    def extract_frames(self, video_path, output_dir, max_frames=None):\n",
        "        \"\"\"Extract frames from a video file\"\"\"\n",
        "        if max_frames is None:\n",
        "            max_frames = self.config.frames_per_video\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            # Open the video file\n",
        "            container = av.open(video_path)\n",
        "\n",
        "            # Get video stream\n",
        "            video_stream = next(s for s in container.streams if s.type == 'video')\n",
        "\n",
        "            # Calculate frame interval to extract evenly distributed frames\n",
        "            n_frames = video_stream.frames\n",
        "            if n_frames <= 0:\n",
        "                # If frames can't be determined, guess based on duration and fps\n",
        "                duration = float(video_stream.duration * video_stream.time_base)\n",
        "                n_frames = int(duration * video_stream.average_rate)\n",
        "\n",
        "            if n_frames <= 0:\n",
        "                print(f\"Warning: Could not determine frame count for {video_path}\")\n",
        "                n_frames = 1000  # Assume a reasonable number\n",
        "\n",
        "            interval = max(1, n_frames // max_frames)\n",
        "\n",
        "            # Extract frames\n",
        "            frame_count = 0\n",
        "            saved_count = 0\n",
        "\n",
        "            for frame in container.decode(video_stream):\n",
        "                if frame_count % interval == 0:\n",
        "                    # Convert to PIL Image\n",
        "                    img = frame.to_image()\n",
        "\n",
        "                    # Save the frame\n",
        "                    img_path = os.path.join(output_dir, f\"frame_{saved_count:04d}.jpg\")\n",
        "                    img.save(img_path)\n",
        "\n",
        "                    saved_count += 1\n",
        "                    if saved_count >= max_frames:\n",
        "                        break\n",
        "\n",
        "                frame_count += 1\n",
        "\n",
        "            return saved_count\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing video {video_path}: {e}\")\n",
        "            return 0\n",
        "\n",
        "    def detect_faces(self, image_path, output_path=None, return_image=False):\n",
        "        \"\"\"Detect faces in an image and save cropped face images\"\"\"\n",
        "        try:\n",
        "            # Load the image\n",
        "            if isinstance(image_path, str):\n",
        "                img = cv2.imread(image_path)\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            else:\n",
        "                img = image_path\n",
        "\n",
        "            # Get face detections\n",
        "            boxes, probs = self.face_detector.detect(img)\n",
        "\n",
        "            if boxes is None or len(boxes) == 0:\n",
        "                return []\n",
        "\n",
        "            # Process each detected face\n",
        "            face_images = []\n",
        "\n",
        "            for i, (box, prob) in enumerate(zip(boxes, probs)):\n",
        "                if prob < 0.9:  # Confidence threshold\n",
        "                    continue\n",
        "\n",
        "                # Get coordinates\n",
        "                x1, y1, x2, y2 = box.tolist()\n",
        "\n",
        "                # Add margin\n",
        "                margin = self.config.face_margin\n",
        "                h, w = img.shape[:2]\n",
        "\n",
        "                dx = (x2 - x1) * margin\n",
        "                dy = (y2 - y1) * margin\n",
        "\n",
        "                x1 = max(0, int(x1 - dx))\n",
        "                y1 = max(0, int(y1 - dy))\n",
        "                x2 = min(w, int(x2 + dx))\n",
        "                y2 = min(h, int(y2 + dy))\n",
        "\n",
        "                # Crop face\n",
        "                face_img = img[y1:y2, x1:x2]\n",
        "\n",
        "                # Save face if requested\n",
        "                if output_path:\n",
        "                    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "                    if len(boxes) == 1:\n",
        "                        face_path = output_path\n",
        "                    else:\n",
        "                        basename = os.path.splitext(output_path)[0]\n",
        "                        ext = os.path.splitext(output_path)[1]\n",
        "                        face_path = f\"{basename}_{i}{ext}\"\n",
        "\n",
        "                    cv2.imwrite(face_path, cv2.cvtColor(face_img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "                if return_image:\n",
        "                    face_images.append(face_img)\n",
        "\n",
        "            return face_images\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error detecting faces in {image_path}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def process_video(self, video_path, output_dir, extract_faces=True):\n",
        "        \"\"\"Process a video: extract frames and detect faces\"\"\"\n",
        "        # Create temporary directory for frames\n",
        "        frames_dir = os.path.join(output_dir, \"frames\")\n",
        "        faces_dir = os.path.join(output_dir, \"faces\")\n",
        "\n",
        "        os.makedirs(frames_dir, exist_ok=True)\n",
        "        if extract_faces:\n",
        "            os.makedirs(faces_dir, exist_ok=True)\n",
        "\n",
        "        # Extract frames\n",
        "        num_frames = self.extract_frames(video_path, frames_dir)\n",
        "\n",
        "        # Detect faces in frames\n",
        "        face_count = 0\n",
        "        if extract_faces and num_frames > 0:\n",
        "            for frame_file in os.listdir(frames_dir):\n",
        "                if not frame_file.endswith(('.jpg', '.png', '.jpeg')):\n",
        "                    continue\n",
        "\n",
        "                frame_path = os.path.join(frames_dir, frame_file)\n",
        "                face_path = os.path.join(faces_dir, frame_file)\n",
        "\n",
        "                _ = self.detect_faces(frame_path, face_path)\n",
        "\n",
        "                if os.path.exists(face_path):\n",
        "                    face_count += 1\n",
        "\n",
        "        return num_frames, face_count"
      ],
      "metadata": {
        "id": "4FZkZ-4pjEgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Dataset Preparation\n",
        "class DeepfakeDatasetPreparation:\n",
        "    \"\"\"Prepare FaceForensics++ and Celeb-DF datasets\"\"\"\n",
        "    def __init__(self, config, device='cuda'):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.processor = VideoProcessor(config, device)\n",
        "\n",
        "    def prepare_ff_dataset(self):\n",
        "        \"\"\"Prepare FaceForensics++ dataset\"\"\"\n",
        "        print(\"Preparing FaceForensics++ dataset...\")\n",
        "\n",
        "        # Define paths\n",
        "        real_dir = os.path.join(self.config.ff_path, \"original_sequences\", \"youtube\",\n",
        "                            self.config.ff_compression, \"videos\")\n",
        "\n",
        "        # Create output directories\n",
        "        output_base = os.path.join(self.config.ff_path, \"processed\")\n",
        "        os.makedirs(output_base, exist_ok=True)\n",
        "\n",
        "        # Process real videos\n",
        "        real_videos = []\n",
        "        if os.path.exists(real_dir):\n",
        "            real_videos = [os.path.join(real_dir, f) for f in os.listdir(real_dir)\n",
        "                          if f.endswith(\".mp4\") or f.endswith(\".avi\")]\n",
        "\n",
        "            # Limit samples if needed\n",
        "            random.shuffle(real_videos)\n",
        "            real_videos = real_videos[:self.config.ff_samples_per_method]\n",
        "\n",
        "            print(f\"Processing {len(real_videos)} real videos...\")\n",
        "            for video_path in tqdm(real_videos):\n",
        "                video_name = os.path.basename(video_path)\n",
        "                output_dir = os.path.join(output_base, \"original\", video_name)\n",
        "                self.processor.process_video(video_path, output_dir)\n",
        "\n",
        "        # Process fake videos for each method\n",
        "        for method in self.config.ff_methods:\n",
        "            fake_dir = os.path.join(self.config.ff_path, \"manipulated_sequences\",\n",
        "                                   method, self.config.ff_compression, \"videos\")\n",
        "\n",
        "            if not os.path.exists(fake_dir):\n",
        "                print(f\"Directory not found: {fake_dir}\")\n",
        "                continue\n",
        "\n",
        "            fake_videos = [os.path.join(fake_dir, f) for f in os.listdir(fake_dir)\n",
        "                          if f.endswith(\".mp4\") or f.endswith(\".avi\")]\n",
        "\n",
        "            # Limit samples if needed\n",
        "            random.shuffle(fake_videos)\n",
        "            fake_videos = fake_videos[:self.config.ff_samples_per_method]\n",
        "\n",
        "            print(f\"Processing {len(fake_videos)} {method} fake videos...\")\n",
        "            for video_path in tqdm(fake_videos):\n",
        "                video_name = os.path.basename(video_path)\n",
        "                output_dir = os.path.join(output_base, method, video_name)\n",
        "                self.processor.process_video(video_path, output_dir)\n",
        "\n",
        "        print(\"FaceForensics++ dataset preparation complete.\")\n",
        "\n",
        "    def prepare_celebdf_dataset(self, version=\"v2\"):\n",
        "        \"\"\"Prepare Celeb-DF dataset\"\"\"\n",
        "        print(f\"Preparing Celeb-DF {version} dataset...\")\n",
        "\n",
        "        # Define paths based on version\n",
        "        if version == \"v2\":\n",
        "            celeb_path = self.config.celebdf_v2_path\n",
        "        else:\n",
        "            celeb_path = self.config.celebdf_path\n",
        "\n",
        "        if not os.path.exists(celeb_path):\n",
        "            print(f\"Celeb-DF {version} path not found: {celeb_path}\")\n",
        "            return\n",
        "\n",
        "        # Create output directories\n",
        "        output_base = os.path.join(celeb_path, \"processed\")\n",
        "        os.makedirs(output_base, exist_ok=True)\n",
        "\n",
        "        # Get real and fake video lists\n",
        "        real_videos = []\n",
        "        fake_videos = []\n",
        "\n",
        "        # Find real videos (usually in Celeb-real or similar directory)\n",
        "        for root, _, files in os.walk(celeb_path):\n",
        "            for file in files:\n",
        "                if file.endswith((\".mp4\", \".avi\")):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    if \"real\" in root.lower():\n",
        "                        real_videos.append(file_path)\n",
        "                    elif \"fake\" in root.lower() or \"synthesis\" in root.lower():\n",
        "                        fake_videos.append(file_path)\n",
        "\n",
        "        # If structured differently, try alternative approach\n",
        "        if len(real_videos) == 0 or len(fake_videos) == 0:\n",
        "            # Some Celeb-DF releases use a txt list file\n",
        "            list_file = os.path.join(celeb_path, \"List_of_testing_videos.txt\")\n",
        "            if os.path.exists(list_file):\n",
        "                with open(list_file, \"r\") as f:\n",
        "                    for line in f:\n",
        "                        parts = line.strip().split()\n",
        "                        if len(parts) >= 2:\n",
        "                            video_path = os.path.join(celeb_path, parts[1])\n",
        "                            if os.path.exists(video_path):\n",
        "                                if \"real\" in parts[1].lower():\n",
        "                                    real_videos.append(video_path)\n",
        "                                else:\n",
        "                                    fake_videos.append(video_path)\n",
        "\n",
        "        # Limit samples if needed\n",
        "        real_limit = min(len(real_videos), self.config.celebdf_samples // 2)\n",
        "        fake_limit = min(len(fake_videos), self.config.celebdf_samples // 2)\n",
        "\n",
        "        random.shuffle(real_videos)\n",
        "        random.shuffle(fake_videos)\n",
        "\n",
        "        real_videos = real_videos[:real_limit]\n",
        "        fake_videos = fake_videos[:fake_limit]\n",
        "\n",
        "        # Process real videos\n",
        "        print(f\"Processing {len(real_videos)} real Celeb-DF videos...\")\n",
        "        for video_path in tqdm(real_videos):\n",
        "            video_name = os.path.basename(video_path)\n",
        "            output_dir = os.path.join(output_base, \"real\", video_name)\n",
        "            self.processor.process_video(video_path, output_dir)\n",
        "\n",
        "        # Process fake videos\n",
        "        print(f\"Processing {len(fake_videos)} fake Celeb-DF videos...\")\n",
        "        for video_path in tqdm(fake_videos):\n",
        "            video_name = os.path.basename(video_path)\n",
        "            output_dir = os.path.join(output_base, \"fake\", video_name)\n",
        "            self.processor.process_video(video_path, output_dir)\n",
        "\n",
        "        print(f\"Celeb-DF {version} dataset preparation complete.\")\n",
        "\n",
        "    def create_dataset_csv(self):\n",
        "        \"\"\"Create CSV files with dataset information\"\"\"\n",
        "        print(\"Creating dataset CSV files...\")\n",
        "\n",
        "        datasets = [\n",
        "            {\"name\": \"ff++\", \"path\": os.path.join(self.config.ff_path, \"processed\")},\n",
        "            {\"name\": \"celebdf\", \"path\": os.path.join(self.config.celebdf_v2_path, \"processed\")}\n",
        "        ]\n",
        "\n",
        "        all_data = []\n",
        "\n",
        "        for dataset in datasets:\n",
        "            if not os.path.exists(dataset[\"path\"]):\n",
        "                print(f\"Dataset path not found: {dataset['path']}\")\n",
        "                continue\n",
        "\n",
        "            # Walk through directories\n",
        "            for root, _, files in os.walk(dataset[\"path\"]):\n",
        "                for file in files:\n",
        "                    if file.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
        "                        file_path = os.path.join(root, file)\n",
        "\n",
        "                        # Determine if real or fake\n",
        "                        is_real = \"original\" in root.lower() or \"real\" in root.lower()\n",
        "                        label = 0 if is_real else 1\n",
        "\n",
        "                        # Determine manipulation method\n",
        "                        if \"deepfakes\" in root.lower():\n",
        "                            method = \"Deepfakes\"\n",
        "                        elif \"face2face\" in root.lower():\n",
        "                            method = \"Face2Face\"\n",
        "                        elif \"faceswap\" in root.lower():\n",
        "                            method = \"FaceSwap\"\n",
        "                        elif \"neuraltextures\" in root.lower():\n",
        "                            method = \"NeuralTextures\"\n",
        "                        elif \"fake\" in root.lower() or \"synthesis\" in root.lower():\n",
        "                            method = \"Celeb-DF\"\n",
        "                        else:\n",
        "                            method = \"Original\"\n",
        "\n",
        "                        all_data.append({\n",
        "                            \"dataset\": dataset[\"name\"],\n",
        "                            \"path\": file_path,\n",
        "                            \"label\": label,\n",
        "                            \"method\": method\n",
        "                        })\n",
        "\n",
        "        # Shuffle data\n",
        "        random.shuffle(all_data)\n",
        "\n",
        "        # Split into train, validation, and test sets\n",
        "        n_samples = len(all_data)\n",
        "        n_train = int(n_samples * self.config.train_ratio)\n",
        "        n_val = int(n_samples * self.config.val_ratio)\n",
        "\n",
        "        train_data = all_data[:n_train]\n",
        "        val_data = all_data[n_train:n_train+n_val]\n",
        "        test_data = all_data[n_train+n_val:]\n",
        "\n",
        "        # Create DataFrames\n",
        "        train_df = pd.DataFrame(train_data)\n",
        "        val_df = pd.DataFrame(val_data)\n",
        "        test_df = pd.DataFrame(test_data)\n",
        "\n",
        "        # Save to CSV\n",
        "        train_df.to_csv(os.path.join(self.config.base_path, \"train_data.csv\"), index=False)\n",
        "        val_df.to_csv(os.path.join(self.config.base_path, \"val_data.csv\"), index=False)\n",
        "        test_df.to_csv(os.path.join(self.config.base_path, \"test_data.csv\"), index=False)\n",
        "\n",
        "        print(f\"Dataset CSV files created: {len(train_data)} train, {len(val_data)} validation, {len(test_data)} test samples\")\n",
        "\n",
        "        return train_df, val_df, test_df"
      ],
      "metadata": {
        "id": "zqwCQCJyjFhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 7: Dataset and Dataloader\n",
        "class DeepfakeDataset(Dataset):\n",
        "    \"\"\"Dataset for deepfake detection\"\"\"\n",
        "    def __init__(self, data_df, transform=None):\n",
        "        self.data_df = data_df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data_df.iloc[idx]\n",
        "\n",
        "        # Load image\n",
        "        try:\n",
        "            image = Image.open(row['path']).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {row['path']}: {e}\")\n",
        "            # Return a default image in case of error\n",
        "            image = Image.new('RGB', (224, 224), color=0)\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            try:\n",
        "                if isinstance(self.transform, A.Compose):\n",
        "                    # For albumentations\n",
        "                    image = np.array(image)\n",
        "                    transformed = self.transform(image=image)\n",
        "                    image = transformed['image']\n",
        "                else:\n",
        "                    # For torchvision transforms\n",
        "                    image = self.transform(image)\n",
        "            except Exception as e:\n",
        "                print(f\"Error applying transform: {e}\")\n",
        "                # Return a default tensor in case of error\n",
        "                image = torch.zeros((3, 224, 224))\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'label': row['label'],\n",
        "            'method': row['method'],\n",
        "            'dataset': row['dataset'],\n",
        "            'path': row['path']\n",
        "        }\n",
        "\n",
        "def get_data_transforms(config):\n",
        "    \"\"\"Create data augmentation pipelines\"\"\"\n",
        "    # Albumentations transforms\n",
        "    train_transform = A.Compose([\n",
        "        A.RandomResizedCrop(config.image_size, config.image_size, scale=(0.8, 1.0)),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.OneOf([\n",
        "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
        "            A.CLAHE(p=0.5),\n",
        "        ], p=0.5),\n",
        "        A.OneOf([\n",
        "            A.GaussNoise(p=0.5),\n",
        "            A.GaussianBlur(blur_limit=3, p=0.5),\n",
        "            A.ImageCompression(quality_lower=50, quality_upper=100, p=0.5),\n",
        "        ], p=0.5),\n",
        "        A.OneOf([\n",
        "            A.Sharpen(p=0.5),\n",
        "            A.Emboss(p=0.5),\n",
        "            A.MotionBlur(p=0.5),\n",
        "        ], p=0.3),\n",
        "        A.RandomBrightnessContrast(p=0.3),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "    val_transform = A.Compose([\n",
        "        A.Resize(config.image_size, config.image_size),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "def create_dataloaders(config, train_df, val_df, test_df):\n",
        "    \"\"\"Create dataloaders for training and evaluation\"\"\"\n",
        "    train_transform, val_transform = get_data_transforms(config)\n",
        "\n",
        "    train_dataset = DeepfakeDataset(train_df, transform=train_transform)\n",
        "    val_dataset = DeepfakeDataset(val_df, transform=val_transform)\n",
        "    test_dataset = DeepfakeDataset(test_df, transform=val_transform)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n"
      ],
      "metadata": {
        "id": "EmIDfTbfjeUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Model Architecture\n",
        "class DeiTForDeepfakeDetection(nn.Module):\n",
        "    \"\"\"Enhanced DeiT model for deepfake detection\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Load base model\n",
        "        self.base_model = create_model(\n",
        "            config.model_name,\n",
        "            pretrained=config.pretrained,\n",
        "            num_classes=0  # Remove classifier\n",
        "        )\n",
        "\n",
        "        # Get feature dimension\n",
        "        self.feature_dim = self.base_model.num_features\n",
        "\n",
        "        # Add attention pooling\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "        # Add classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(self.feature_dim),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(self.feature_dim, 512),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, config.num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize newly added layers\"\"\"\n",
        "        for module in [self.attention, self.classifier]:\n",
        "            for m in module.modules():\n",
        "                if isinstance(m, nn.Linear):\n",
        "                    nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                    if m.bias is not None:\n",
        "                        nn.init.constant_(m.bias, 0)\n",
        "                elif isinstance(m, nn.LayerNorm):\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "                    nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        # Extract base features\n",
        "        features = self.base_model(x)\n",
        "\n",
        "        # Apply attention pooling\n",
        "        attention_weights = self.attention(features)\n",
        "        attention_weights = F.softmax(attention_weights, dim=1)\n",
        "        attended_features = (features * attention_weights).sum(dim=1)\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(attended_features)\n",
        "\n",
        "        if return_features:\n",
        "            return logits, features, attention_weights\n",
        "        return logits"
      ],
      "metadata": {
        "id": "PgNimqiPjnGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Training and Evaluation Functions\n",
        "class DeepfakeTrainer:\n",
        "    \"\"\"Trainer for the deepfake detection model\"\"\"\n",
        "    def __init__(self, model, train_loader, val_loader, test_loader, config, device):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        # Move model to device\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=config.learning_rate,\n",
        "            weight_decay=config.weight_decay\n",
        "        )\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        self.scheduler = self._get_scheduler()\n",
        "\n",
        "        # Loss function\n",
        "        self.criterion = nn.CrossEntropyLoss(label_smoothing=config.label_smoothing)\n",
        "\n",
        "        # Gradient scaler for mixed precision training\n",
        "        self.scaler = GradScaler() if config.mixed_precision else None\n",
        "\n",
        "        # Best metrics for model saving\n",
        "        self.best_val_auc = 0.0\n",
        "        self.best_epoch = 0\n",
        "\n",
        "    def _get_scheduler(self):\n",
        "        \"\"\"Get learning rate scheduler\"\"\"\n",
        "        if self.config.warmup_epochs > 0:\n",
        "            # LinearWarmupCosineAnnealingLR\n",
        "            return optim.lr_scheduler.OneCycleLR(\n",
        "                self.optimizer,\n",
        "                max_lr=self.config.learning_rate,\n",
        "                total_steps=self.config.num_epochs * len(self.train_loader),\n",
        "                pct_start=self.config.warmup_epochs / self.config.num_epochs,\n",
        "                div_factor=25.0,\n",
        "                final_div_factor=1000.0,\n",
        "                anneal_strategy='cos'\n",
        "            )\n",
        "        else:\n",
        "            # CosineAnnealingLR\n",
        "            return optim.lr_scheduler.CosineAnnealingLR(\n",
        "                self.optimizer,\n",
        "                T_max=self.config.num_epochs\n",
        "            )\n",
        "\n",
        "    def mixup_data(self, x, y, alpha=0.2):\n",
        "        \"\"\"Apply mixup augmentation\"\"\"\n",
        "        if alpha > 0:\n",
        "            lam = np.random.beta(alpha, alpha)\n",
        "        else:\n",
        "            lam = 1\n",
        "\n",
        "        batch_size = x.size()[0]\n",
        "        index = torch.randperm(batch_size).to(self.device)\n",
        "\n",
        "        mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "        y_a, y_b = y, y[index]\n",
        "\n",
        "        return mixed_x, y_a, y_b, lam\n",
        "\n",
        "    def train_epoch(self, epoch):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, batch in enumerate(tqdm(self.train_loader, desc=f\"Epoch {epoch+1} Training\")):\n",
        "            x = batch['image'].to(self.device)\n",
        "            y = batch['label'].to(self.device)\n",
        "\n",
        "            # Apply mixup if configured\n",
        "            do_mixup = self.config.mixup_alpha > 0 and np.random.random() < 0.5\n",
        "            if do_mixup:\n",
        "                x, y_a, y_b, lam = self.mixup_data(x, y, self.config.mixup_alpha)\n",
        "                y_a, y_b = y_a.long(), y_b.long()\n",
        "\n",
        "            # Gradient accumulation steps\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            if self.config.mixed_precision:\n",
        "                with autocast():\n",
        "                    outputs = self.model(x)\n",
        "\n",
        "                    if do_mixup:\n",
        "                        loss = lam * self.criterion(outputs, y_a) + (1 - lam) * self.criterion(outputs, y_b)\n",
        "                    else:\n",
        "                        loss = self.criterion(outputs, y)\n",
        "\n",
        "                # Scale the loss and backpropagate\n",
        "                self.scaler.scale(loss).backward()\n",
        "\n",
        "                # Clip gradients if configured\n",
        "                if self.config.gradient_clip_val > 0:\n",
        "                    self.scaler.unscale_(self.optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_val)\n",
        "\n",
        "                # Update weights\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "            else:\n",
        "                # Standard training without mixed precision\n",
        "                outputs = self.model(x)\n",
        "\n",
        "                if do_mixup:\n",
        "                    loss = lam * self.criterion(outputs, y_a) + (1 - lam) * self.criterion(outputs, y_b)\n",
        "                else:\n",
        "                    loss = self.criterion(outputs, y)\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "\n",
        "                # Clip gradients if configured\n",
        "                if self.config.gradient_clip_val > 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_val)\n",
        "\n",
        "                # Update weights\n",
        "                self.optimizer.step()\n",
        "\n",
        "            # Update learning rate if step-based scheduler\n",
        "            if isinstance(self.scheduler, optim.lr_scheduler.OneCycleLR):\n",
        "                self.scheduler.step()\n",
        "\n",
        "            # Update metrics\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            if not do_mixup:\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += y.size(0)\n",
        "                correct += predicted.eq(y).sum().item()\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        train_loss = train_loss / len(self.train_loader)\n",
        "        train_acc = correct / total if total > 0 else 0\n",
        "\n",
        "        return train_loss, train_acc\n",
        "\n",
        "    def validate(self, loader, is_test=False):\n",
        "        \"\"\"Validate or test the model\"\"\"\n",
        "        self.model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        all_preds = []\n",
        "        all_probs = []\n",
        "        all_labels = []\n",
        "        all_methods = []\n",
        "        all_datasets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(loader, desc=\"Testing\" if is_test else \"Validation\"):\n",
        "                x = batch['image'].to(self.device)\n",
        "                y = batch['label'].to(self.device)\n",
        "                methods = batch['method']\n",
        "                datasets = batch['dataset']\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(x)\n",
        "                loss = self.criterion(outputs, y)\n",
        "\n",
        "                # Calculate probabilities\n",
        "                probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "                # Get predictions\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                # Update metrics\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Store predictions and labels\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_probs.extend(probs[:, 1].cpu().numpy())  # Probability of fake class\n",
        "                all_labels.extend(y.cpu().numpy())\n",
        "                all_methods.extend(methods)\n",
        "                all_datasets.extend(datasets)\n",
        "\n",
        "        # Calculate metrics\n",
        "        val_loss = val_loss / len(loader)\n",
        "        metrics = self.calculate_metrics(all_labels, all_preds, all_probs)\n",
        "\n",
        "        # Calculate per-method and per-dataset metrics\n",
        "        method_metrics = self.calculate_group_metrics(all_labels, all_preds, all_probs, all_methods)\n",
        "        dataset_metrics = self.calculate_group_metrics(all_labels, all_preds, all_probs, all_datasets)\n",
        "\n",
        "        return val_loss, metrics, method_metrics, dataset_metrics\n",
        "\n",
        "    def calculate_metrics(self, y_true, y_pred, y_prob):\n",
        "        \"\"\"Calculate various metrics\"\"\"\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(y_true, y_pred),\n",
        "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "            'f1': f1_score(y_true, y_pred, zero_division=0),\n",
        "            'auc': roc_auc_score(y_true, y_prob) if len(set(y_true)) > 1 else 0.5\n",
        "        }\n",
        "\n",
        "        # Calculate confusion matrix\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
        "        metrics['tn'] = tn\n",
        "        metrics['fp'] = fp\n",
        "        metrics['fn'] = fn\n",
        "        metrics['tp'] = tp\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def calculate_group_metrics(self, y_true, y_pred, y_prob, groups):\n",
        "        \"\"\"Calculate metrics per group (method or dataset)\"\"\"\n",
        "        group_metrics = {}\n",
        "\n",
        "        for group in set(groups):\n",
        "            # Get indices for this group\n",
        "            indices = [i for i, g in enumerate(groups) if g == group]\n",
        "\n",
        "            if len(indices) == 0:\n",
        "                continue\n",
        "\n",
        "            # Get labels, predictions, and probabilities for this group\n",
        "            group_true = [y_true[i] for i in indices]\n",
        "            group_pred = [y_pred[i] for i in indices]\n",
        "            group_prob = [y_prob[i] for i in indices]\n",
        "\n",
        "            # Calculate metrics\n",
        "            try:\n",
        "                group_metrics[group] = self.calculate_metrics(group_true, group_pred, group_prob)\n",
        "                group_metrics[group]['count'] = len(indices)\n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating metrics for group {group}: {e}\")\n",
        "                group_metrics[group] = {'count': len(indices), 'error': str(e)}\n",
        "\n",
        "        return group_metrics\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        print(f\"Starting training for {self.config.num_epochs} epochs...\")\n",
        "\n",
        "        # Initialize metrics storage\n",
        "        history = {\n",
        "            'train_loss': [], 'train_acc': [],\n",
        "            'val_loss': [], 'val_metrics': [],\n",
        "            'method_metrics': [], 'dataset_metrics': []\n",
        "        }\n",
        "\n",
        "        for epoch in range(self.config.num_epochs):\n",
        "            # Train\n",
        "            train_loss, train_acc = self.train_epoch(epoch)\n",
        "\n",
        "            # Validate\n",
        "            val_loss, val_metrics, method_metrics, dataset_metrics = self.validate(self.val_loader)\n",
        "\n",
        "            # Update epoch-based scheduler\n",
        "            if isinstance(self.scheduler, optim.lr_scheduler.CosineAnnealingLR):\n",
        "                self.scheduler.step()\n",
        "\n",
        "            # Update history\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['train_acc'].append(train_acc)\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['val_metrics'].append(val_metrics)\n",
        "            history['method_metrics'].append(method_metrics)\n",
        "            history['dataset_metrics'].append(dataset_metrics)\n",
        "\n",
        "            # Print metrics\n",
        "            print(f\"Epoch {epoch+1}/{self.config.num_epochs}:\")\n",
        "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "            print(f\"  Val Loss: {val_loss:.4f}, Val AUC: {val_metrics['auc']:.4f}\")\n",
        "\n",
        "            # Save checkpoint if it's the best model so far\n",
        "            if val_metrics['auc'] > self.best_val_auc:\n",
        "                self.best_val_auc = val_metrics['auc']\n",
        "                self.best_epoch = epoch\n",
        "\n",
        "                # Save model\n",
        "                checkpoint_path = os.path.join(self.config.checkpoint_dir, \"best_model.pth\")\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'val_metrics': val_metrics,\n",
        "                    'config': self.config.__dict__\n",
        "                }, checkpoint_path)\n",
        "\n",
        "                print(f\"  New best model saved with AUC: {self.best_val_auc:.4f}\")\n",
        "\n",
        "            # Save regular checkpoint\n",
        "            if (epoch + 1) % self.config.save_frequency == 0:\n",
        "                checkpoint_path = os.path.join(self.config.checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'val_metrics': val_metrics,\n",
        "                    'config': self.config.__dict__\n",
        "                }, checkpoint_path)\n",
        "\n",
        "        # Save final model\n",
        "        checkpoint_path = os.path.join(self.config.checkpoint_dir, \"final_model.pth\")\n",
        "        torch.save({\n",
        "            'epoch': self.config.num_epochs - 1,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'val_metrics': history['val_metrics'][-1],\n",
        "            'config': self.config.__dict__\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        print(f\"Training completed. Best model at epoch {self.best_epoch+1} with AUC: {self.best_val_auc:.4f}\")\n",
        "\n",
        "        return history\n",
        "\n",
        "    def test(self, model_path=None):\n",
        "        \"\"\"Test the model on the test set\"\"\"\n",
        "        if model_path:\n",
        "            # Load the model\n",
        "            checkpoint = torch.load(model_path, map_location=self.device)\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        # Run validation on test set\n",
        "        test_loss, test_metrics, method_metrics, dataset_metrics = self.validate(self.test_loader, is_test=True)\n",
        "\n",
        "        print(f\"Test Results:\")\n",
        "        print(f\"  Loss: {test_loss:.4f}\")\n",
        "        print(f\"  Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "        print(f\"  AUC: {test_metrics['auc']:.4f}\")\n",
        "        print(f\"  F1 Score: {test_metrics['f1']:.4f}\")\n",
        "\n",
        "        # Save results\n",
        "        results = {\n",
        "            'test_loss': test_loss,\n",
        "            'test_metrics': test_metrics,\n",
        "            'method_metrics': method_metrics,\n",
        "            'dataset_metrics': dataset_metrics\n",
        "        }\n",
        "\n",
        "        results_path = os.path.join(self.config.results_dir, \"test_results.json\")\n",
        "        with open(results_path, 'w') as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "H7UuurmepeKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Visualization Functions\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training history\"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(history['train_acc'], label='Train Acc')\n",
        "    val_acc = [metrics['accuracy'] for metrics in history['val_metrics']]\n",
        "    plt.plot(val_acc, label='Val Acc')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot AUC\n",
        "    plt.subplot(1, 3, 3)\n",
        "    val_auc = [metrics['auc'] for metrics in history['val_metrics']]\n",
        "    plt.plot(val_auc, label='Val AUC')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('AUC')\n",
        "    plt.title('Validation AUC')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes=['Real', 'Fake']):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc_curve(y_true, y_score):\n",
        "    \"\"\"Plot ROC curve\"\"\"\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_method_performance(method_metrics):\n",
        "    \"\"\"Plot performance by manipulation method\"\"\"\n",
        "    methods = list(method_metrics.keys())\n",
        "    accs = [method_metrics[m]['accuracy'] for m in methods]\n",
        "    aucs = [method_metrics[m]['auc'] for m in methods]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot accuracies\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.bar(methods, accs, color='skyblue')\n",
        "    plt.ylim([0, 1])\n",
        "    plt.xlabel('Method')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy by Manipulation Method')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Plot AUCs\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar(methods, aucs, color='lightgreen')\n",
        "    plt.ylim([0, 1])\n",
        "    plt.xlabel('Method')\n",
        "    plt.ylabel('AUC')\n",
        "    plt.title('AUC by Manipulation Method')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_attention(model, image_path, transform, device):\n",
        "    \"\"\"Visualize attention maps\"\"\"\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    orig_image = np.array(image)\n",
        "\n",
        "    # Apply transform\n",
        "    if isinstance(transform, A.Compose):\n",
        "        transformed = transform(image=np.array(image))\n",
        "        tensor = transformed['image']\n",
        "    else:\n",
        "        tensor = transform(image)\n",
        "\n",
        "    # Add batch dimension\n",
        "    tensor = tensor.unsqueeze(0).to(device)\n",
        "\n",
        "    # Get predictions and attention weights\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits, features, attention_weights = model(tensor, return_features=True)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        pred_class = torch.argmax(probs, dim=1).item()\n",
        "        pred_prob = probs[0, pred_class].item()\n",
        "\n",
        "        # Get attention weights\n",
        "        attention = attention_weights.squeeze().cpu().numpy()\n",
        "\n",
        "    # Create heatmap\n",
        "    attention = attention.reshape(-1)\n",
        "    attention = (attention - attention.min()) / (attention.max() - attention.min())\n",
        "\n",
        "    # Resize image for visualization\n",
        "    h, w = orig_image.shape[:2]\n",
        "    resized = cv2.resize(orig_image, (224, 224))\n",
        "\n",
        "    # Normalize attention for visualization\n",
        "    attention_map = attention.reshape(1, -1)\n",
        "    attention_map = (attention_map - attention_map.min()) / (attention_map.max() - attention_map.min())\n",
        "    attention_map = attention_map.reshape(14, 14)  # Adjust based on model patch size\n",
        "    attention_map = cv2.resize(attention_map, (224, 224))\n",
        "\n",
        "    # Apply colormap\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * attention_map), cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Overlay heatmap on image\n",
        "    alpha = 0.5\n",
        "    overlay = alpha * heatmap + (1 - alpha) * resized\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(resized)\n",
        "    plt.title(f\"Original Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(attention_map, cmap='hot')\n",
        "    plt.title(\"Attention Map\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(overlay.astype(np.uint8))\n",
        "    plt.title(f\"Overlay (Pred: {'Fake' if pred_class == 1 else 'Real'}, {pred_prob:.2f})\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6vZHJllDpjKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Model Deployment Utils\n",
        "def export_to_onnx(model, save_path, input_shape=(1, 3, 224, 224), device='cuda'):\n",
        "    \"\"\"Export model to ONNX format for deployment\"\"\"\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(input_shape).to(device)\n",
        "\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        dummy_input,\n",
        "        save_path,\n",
        "        export_params=True,\n",
        "        opset_version=13,\n",
        "        do_constant_folding=True,\n",
        "        input_names=['input'],\n",
        "        output_names=['output'],\n",
        "        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
        "    )\n",
        "\n",
        "    print(f\"Model exported to ONNX format: {save_path}\")\n",
        "\n",
        "def inference_pipeline(image_path, model, transform, device='cuda'):\n",
        "    \"\"\"Complete inference pipeline for a single image\"\"\"\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    # Apply transform\n",
        "    if isinstance(transform, A.Compose):\n",
        "        transformed = transform(image=np.array(image))\n",
        "        tensor = transformed['image']\n",
        "    else:\n",
        "        tensor = transform(image)\n",
        "\n",
        "    # Add batch dimension\n",
        "    tensor = tensor.unsqueeze(0).to(device)\n",
        "\n",
        "    # Get predictions\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(tensor)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        pred_class = torch.argmax(probs, dim=1).item()\n",
        "        pred_prob = probs[0, pred_class].item()\n",
        "\n",
        "    result = {\n",
        "        'path': image_path,\n",
        "        'prediction': 'Fake' if pred_class == 1 else 'Real',\n",
        "        'confidence': pred_prob,\n",
        "        'fake_probability': probs[0, 1].item()\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "XvUfnXKBqAkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Main Execution Script\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Configuration\n",
        "    config = Config()\n",
        "\n",
        "    # Step 1: Dataset Preparation (optional, can be commented out if already done)\n",
        "    \"\"\"\n",
        "    print(\"Step 1: Preparing datasets...\")\n",
        "    preparation = DeepfakeDatasetPreparation(config, device)\n",
        "\n",
        "    # Process FaceForensics++\n",
        "    preparation.prepare_ff_dataset()\n",
        "\n",
        "    # Process Celeb-DF\n",
        "    preparation.prepare_celebdf_dataset(version=\"v2\")\n",
        "\n",
        "    # Create dataset CSV files\n",
        "    train_df, val_df, test_df = preparation.create_dataset_csv()\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 2: Load dataset CSVs\n",
        "    print(\"Step 2: Loading dataset CSVs...\")\n",
        "    train_df = pd.read_csv(os.path.join(config.base_path, \"train_data.csv\"))\n",
        "    val_df = pd.read_csv(os.path.join(config.base_path, \"val_data.csv\"))\n",
        "    test_df = pd.read_csv(os.path.join(config.base_path, \"test_data.csv\"))\n",
        "\n",
        "    print(f\"Dataset loaded: {len(train_df)} train, {len(val_df)} validation, {len(test_df)} test samples\")\n",
        "\n",
        "    # Step 3: Create dataloaders\n",
        "    print(\"Step 3: Creating dataloaders...\")\n",
        "    train_loader, val_loader, test_loader = create_dataloaders(config, train_df, val_df, test_df)\n",
        "\n",
        "    # Step 4: Create model\n",
        "    print(\"Step 4: Creating model...\")\n",
        "    model = DeiTForDeepfakeDetection(config).to(device)\n",
        "\n",
        "    # Print model summary\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Model created with {total_params:,} total parameters, {trainable_params:,} trainable\")\n",
        "\n",
        "    # Step 5: Train model\n",
        "    print(\"Step 5: Training model...\")\n",
        "    trainer = DeepfakeTrainer(model, train_loader, val_loader, test_loader, config, device)\n",
        "    history = trainer.train()\n",
        "\n",
        "    # Step 6: Evaluate on test set\n",
        "    print(\"Step 6: Evaluating on test set...\")\n",
        "    best_model_path = os.path.join(config.checkpoint_dir, \"best_model.pth\")\n",
        "    test_results = trainer.test(best_model_path)\n",
        "\n",
        "    # Step 7: Visualize results\n",
        "    print(\"Step 7: Visualizing results...\")\n",
        "    plot_training_history(history)\n",
        "\n",
        "    # Get test predictions\n",
        "    checkpoint = torch.load(best_model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Get predictions on test set\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Getting test predictions\"):\n",
        "            x = batch['image'].to(device)\n",
        "            y = batch['label']\n",
        "\n",
        "            outputs = model(x)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
        "            all_labels.extend(y.numpy())\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plot_confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plot_roc_curve(all_labels, all_probs)\n",
        "\n",
        "    # Plot method performance\n",
        "    plot_method_performance(test_results['method_metrics'])\n",
        "\n",
        "    # Step 8: Export model for deployment\n",
        "    print(\"Step 8: Exporting model for deployment...\")\n",
        "    onnx_path = os.path.join(config.results_dir, \"deepfake_detector.onnx\")\n",
        "    export_to_onnx(model, onnx_path, device=device)\n",
        "\n",
        "    print(\"Complete pipeline execution finished successfully!\")\n",
        "    return model, history, test_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "kcfaVhoYqBWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Additional Testing and Visualization\n",
        "def test_on_sample_images(model, config, device='cuda'):\n",
        "    \"\"\"Test model on sample images and visualize results\"\"\"\n",
        "    # Load the model if not already loaded\n",
        "    if isinstance(model, str):\n",
        "        model = DeiTForDeepfakeDetection(config).to(device)\n",
        "        checkpoint = torch.load(model, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Get transform\n",
        "    _, transform = get_data_transforms(config)\n",
        "\n",
        "    # Test directories\n",
        "    test_dirs = [\n",
        "        # FaceForensics++ samples\n",
        "        os.path.join(config.ff_path, \"processed/original\"),\n",
        "        os.path.join(config.ff_path, \"processed/Deepfakes\"),\n",
        "        os.path.join(config.ff_path, \"processed/Face2Face\"),\n",
        "        os.path.join(config.ff_path, \"processed/FaceSwap\"),\n",
        "        os.path.join(config.ff_path, \"processed/NeuralTextures\"),\n",
        "        # Celeb-DF samples\n",
        "        os.path.join(config.celebdf_v2_path, \"processed/real\"),\n",
        "        os.path.join(config.celebdf_v2_path, \"processed/fake\")\n",
        "    ]\n",
        "\n",
        "    # Results storage\n",
        "    results = []\n",
        "\n",
        "    # Test on random samples from each directory\n",
        "    for test_dir in test_dirs:\n",
        "        if not os.path.exists(test_dir):\n",
        "            continue\n",
        "\n",
        "        # Find all image files\n",
        "        image_files = []\n",
        "        for root, _, files in os.walk(test_dir):\n",
        "            for file in files:\n",
        "                if file.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
        "                    image_files.append(os.path.join(root, file))\n",
        "\n",
        "        if len(image_files) == 0:\n",
        "            continue\n",
        "\n",
        "        # Select random samples\n",
        "        samples = random.sample(image_files, min(5, len(image_files)))\n",
        "\n",
        "        for image_path in samples:\n",
        "            # Run inference\n",
        "            result = inference_pipeline(image_path, model, transform, device)\n",
        "\n",
        "            # Determine true label from path\n",
        "            true_label = \"Fake\"\n",
        "            if \"original\" in image_path.lower() or \"real\" in image_path.lower():\n",
        "                true_label = \"Real\"\n",
        "\n",
        "            # Add to results\n",
        "            result['true_label'] = true_label\n",
        "            result['dir'] = os.path.basename(test_dir)\n",
        "            results.append(result)\n",
        "\n",
        "            # Visualize attention\n",
        "            visualize_attention(model, image_path, transform, device)\n",
        "\n",
        "            # Print result\n",
        "            print(f\"Image: {os.path.basename(image_path)}\")\n",
        "            print(f\"Directory: {result['dir']}\")\n",
        "            print(f\"True label: {result['true_label']}\")\n",
        "            print(f\"Prediction: {result['prediction']} (Confidence: {result['confidence']:.4f})\")\n",
        "            print(f\"Fake probability: {result['fake_probability']:.4f}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    correct = sum(1 for r in results if r['prediction'] == r['true_label'])\n",
        "    accuracy = correct / len(results)\n",
        "\n",
        "    print(f\"Overall accuracy on sample images: {accuracy:.4f} ({correct}/{len(results)})\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "uL2ZsqprqbMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Cross-Dataset Evaluation\n",
        "def cross_dataset_evaluation(model, config, device='cuda'):\n",
        "    \"\"\"Evaluate model trained on one dataset on another dataset\"\"\"\n",
        "    # Load the model if not already loaded\n",
        "    if isinstance(model, str):\n",
        "        model = DeiTForDeepfakeDetection(config).to(device)\n",
        "        checkpoint = torch.load(model, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Get transform\n",
        "    _, transform = get_data_transforms(config)\n",
        "\n",
        "    # Load dataset CSVs\n",
        "    all_data = pd.read_csv(os.path.join(config.base_path, \"test_data.csv\"))\n",
        "\n",
        "    # Split by dataset\n",
        "    ff_data = all_data[all_data['dataset'] == 'ff++']\n",
        "    celebdf_data = all_data[all_data['dataset'] == 'celebdf']\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    ff_dataset = DeepfakeDataset(ff_data, transform=transform)\n",
        "    celebdf_dataset = DeepfakeDataset(celebdf_data, transform=transform)\n",
        "\n",
        "    ff_loader = DataLoader(\n",
        "        ff_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    celebdf_loader = DataLoader(\n",
        "        celebdf_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Evaluate on both datasets\n",
        "    print(\"Evaluating on FaceForensics++...\")\n",
        "    ff_results = {}\n",
        "    if len(ff_data) > 0:\n",
        "        trainer = DeepfakeTrainer(model, None, None, ff_loader, config, device)\n",
        "        ff_loss, ff_metrics, ff_method_metrics, _ = trainer.validate(ff_loader, is_test=True)\n",
        "        ff_results = {\n",
        "            'loss': ff_loss,\n",
        "            'metrics': ff_metrics,\n",
        "            'method_metrics': ff_method_metrics\n",
        "        }\n",
        "        print(f\"  Loss: {ff_loss:.4f}, AUC: {ff_metrics['auc']:.4f}\")\n",
        "\n",
        "    print(\"Evaluating on Celeb-DF...\")\n",
        "    celebdf_results = {}\n",
        "    if len(celebdf_data) > 0:\n",
        "        trainer = DeepfakeTrainer(model, None, None, celebdf_loader, config, device)\n",
        "        celebdf_loss, celebdf_metrics, celebdf_method_metrics, _ = trainer.validate(celebdf_loader, is_test=True)\n",
        "        celebdf_results = {\n",
        "            'loss': celebdf_loss,\n",
        "            'metrics': celebdf_metrics,\n",
        "            'method_metrics': celebdf_method_metrics\n",
        "        }\n",
        "        print(f\"  Loss: {celebdf_loss:.4f}, AUC: {celebdf_metrics['auc']:.4f}\")\n",
        "\n",
        "    # Compare results\n",
        "    results = {\n",
        "        'ff++': ff_results,\n",
        "        'celebdf': celebdf_results\n",
        "    }\n",
        "\n",
        "    # Save results\n",
        "    results_path = os.path.join(config.results_dir, \"cross_dataset_results.json\")\n",
        "    with open(results_path, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    # Plot"
      ],
      "metadata": {
        "id": "z7D-JO9wqxxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utility functions for deepfake detection project\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from facenet_pytorch import MTCNN\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"Setup the environment for Google Colab\"\"\"\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Install required packages\n",
        "    packages = [\n",
        "        \"timm==0.6.12\",\n",
        "        \"opencv-python==4.8.0.76\",\n",
        "        \"albumentations==1.3.1\",\n",
        "        \"facenet-pytorch==2.5.3\",\n",
        "        \"wandb==0.15.12\",\n",
        "        \"onnx==1.14.1\",\n",
        "        \"av==10.0.0\",\n",
        "        \"ffmpeg-python==0.2.0\",\n",
        "        \"scikit-learn==1.3.0\",\n",
        "        \"tqdm==4.66.1\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        !pip install {package}\n",
        "\n",
        "    # Set random seeds for reproducibility\n",
        "    import random\n",
        "    import numpy as np\n",
        "    import torch\n",
        "\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    print(\"Environment setup completed\")\n",
        "\n",
        "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def detect_and_crop_face(image_path, face_detector=None, output_path=None, margin=0.3):\n",
        "    \"\"\"Detect and crop a face from an image\"\"\"\n",
        "    if face_detector is None:\n",
        "        face_detector = MTCNN(\n",
        "            keep_all=False,\n",
        "            post_process=False,\n",
        "            min_face_size=48,\n",
        "            device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        )\n",
        "\n",
        "    # Load image\n",
        "    if isinstance(image_path, str):\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"Image not found: {image_path}\")\n",
        "            return None\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    else:\n",
        "        image = image_path\n",
        "\n",
        "    # Detect faces\n",
        "    try:\n",
        "        boxes, probs = face_detector.detect(image)\n",
        "\n",
        "        if boxes is None or len(boxes) == 0:\n",
        "            print(f\"No face detected in {image_path}\")\n",
        "            return None\n",
        "\n",
        "        # Get the face with highest probability\n",
        "        box = boxes[0]\n",
        "\n",
        "        # Add margin\n",
        "        h, w = image.shape[:2]\n",
        "        x1, y1, x2, y2 = box.tolist()\n",
        "\n",
        "        dx = (x2 - x1) * margin\n",
        "        dy = (y2 - y1) * margin\n",
        "\n",
        "        x1 = max(0, int(x1 - dx))\n",
        "        y1 = max(0, int(y1 - dy))\n",
        "        x2 = min(w, int(x2 + dx))\n",
        "        y2 = min(h, int(y2 + dy))\n",
        "\n",
        "        # Crop face\n",
        "        face_img = image[y1:y2, x1:x2]\n",
        "\n",
        "        # Save if requested\n",
        "        if output_path:\n",
        "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "            cv2.imwrite(output_path, cv2.cvtColor(face_img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        return face_img\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error detecting face in {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_default_transforms(image_size=224):\n",
        "    \"\"\"Get default transforms for training and validation\"\"\"\n",
        "    train_transform = A.Compose([\n",
        "        A.RandomResizedCrop(image_size, image_size, scale=(0.8, 1.0)),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.OneOf([\n",
        "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
        "            A.CLAHE(p=0.5),\n",
        "        ], p=0.5),\n",
        "        A.OneOf([\n",
        "            A.GaussNoise(p=0.5),\n",
        "            A.GaussianBlur(blur_limit=3, p=0.5),\n",
        "            A.ImageCompression(quality_lower=50, quality_upper=100, p=0.5),\n",
        "        ], p=0.5),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "    val_transform = A.Compose([\n",
        "        A.Resize(image_size, image_size),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "def predict_image(model, image_path, transform=None, device=None, face_detector=None):\n",
        "    \"\"\"Predict if an image is real or fake\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if transform is None:\n",
        "        _, transform = get_default_transforms()\n",
        "\n",
        "    # Load model if it's a path\n",
        "    if isinstance(model, str):\n",
        "        from model import DeiTForDeepfakeDetection\n",
        "        model_instance = DeiTForDeepfakeDetection(num_classes=2).to(device)\n",
        "        checkpoint = torch.load(model, map_location=device)\n",
        "        model_instance.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model = model_instance\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Process image\n",
        "    if face_detector is not None:\n",
        "        # Detect and crop face\n",
        "        face_img = detect_and_crop_face(image_path, face_detector)\n",
        "        if face_img is None:\n",
        "            return {\n",
        "                'error': 'No face detected',\n",
        "                'prediction': None,\n",
        "                'confidence': 0.0,\n",
        "                'probabilities': [0.0, 0.0]\n",
        "            }\n",
        "    else:\n",
        "        # Just load the image\n",
        "        try:\n",
        "            face_img = Image.open(image_path).convert('RGB')\n",
        "            face_img = np.array(face_img)\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'error': f'Error loading image: {e}',\n",
        "                'prediction': None,\n",
        "                'confidence': 0.0,\n",
        "                'probabilities': [0.0, 0.0]\n",
        "            }\n",
        "\n",
        "    # Apply transform\n",
        "    try:\n",
        "        transformed = transform(image=face_img)\n",
        "        tensor = transformed['image'].unsqueeze(0).to(device)\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'error': f'Error applying transform: {e}',\n",
        "            'prediction': None,\n",
        "            'confidence': 0.0,\n",
        "            'probabilities': [0.0, 0.0]\n",
        "        }\n",
        "\n",
        "    # Get prediction\n",
        "    with torch.no_grad():\n",
        "        logits = model(tensor)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        pred_class = torch.argmax(probs, dim=1).item()\n",
        "        pred_prob = probs[0, pred_class].item()\n",
        "\n",
        "    result = {\n",
        "        'prediction': 'Fake' if pred_class == 1 else 'Real',\n",
        "        'confidence': pred_prob,\n",
        "        'probabilities': probs[0].cpu().numpy().tolist()\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "def visualize_prediction(image_path, result, face_detector=None):\n",
        "    \"\"\"Visualize prediction result on an image\"\"\"\n",
        "    # Load image\n",
        "    try:\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image: {e}\")\n",
        "        return\n",
        "\n",
        "    # Detect face if detector is provided\n",
        "    if face_detector is not None:\n",
        "        face_img = detect_and_crop_face(image_path, face_detector)\n",
        "        if face_img is not None:\n",
        "            image = face_img\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Display image\n",
        "    plt.imshow(image)\n",
        "\n",
        "    # Add prediction text\n",
        "    if 'error' in result and result['error']:\n",
        "        plt.title(f\"Error: {result['error']}\", color='red', fontsize=14)\n",
        "    else:\n",
        "        pred = result['prediction']\n",
        "        conf = result['confidence']\n",
        "        color = 'green' if pred == 'Real' else 'red'\n",
        "        plt.title(f\"Prediction: {pred} (Confidence: {conf:.4f})\", color=color, fontsize=14)\n",
        "\n",
        "        # Add probabilities\n",
        "        if 'probabilities' in result:\n",
        "            real_prob = result['probabilities'][0]\n",
        "            fake_prob = result['probabilities'][1]\n",
        "            plt.xlabel(f\"Real: {real_prob:.4f}, Fake: {fake_prob:.4f}\", fontsize=12)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def extract_frames_from_video(video_path, output_dir, num_frames=20):\n",
        "    \"\"\"Extract frames from a video file\"\"\"\n",
        "    import av\n",
        "\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"Video not found: {video_path}\")\n",
        "        return False\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # Open the video\n",
        "        container = av.open(video_path)\n",
        "        video_stream = next(s for s in container.streams if s.type == 'video')\n",
        "\n",
        "        # Get total frames\n",
        "        n_frames = video_stream.frames\n",
        "        if n_frames <= 0:\n",
        "            # Estimate using duration and framerate\n",
        "            duration = float(video_stream.duration * video_stream.time_base)\n",
        "            n_frames = int(duration * video_stream.average_rate)\n",
        "\n",
        "        if n_frames <= 0:\n",
        "            print(f\"Could not determine number of frames for {video_path}\")\n",
        "            n_frames = 1000  # Assume a reasonable number\n",
        "\n",
        "        # Calculate interval\n",
        "        interval = max(1, n_frames // num_frames)\n",
        "\n",
        "        # Extract frames\n",
        "        frame_count = 0\n",
        "        saved_count = 0\n",
        "\n",
        "        for frame in container.decode(video_stream):\n",
        "            if frame_count % interval == 0:\n",
        "                # Save frame\n",
        "                img = frame.to_image()\n",
        "                img_path = os.path.join(output_dir, f\"frame_{saved_count:04d}.jpg\")\n",
        "                img.save(img_path)\n",
        "\n",
        "                saved_count += 1\n",
        "                if saved_count >= num_frames:\n",
        "                    break\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "        print(f\"Extracted {saved_count} frames from {video_path}\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting frames from {video_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "def batch_process_frames(model, frames_dir, transform=None, device=None, face_detector=None):\n",
        "    \"\"\"Process all frames in a directory and return results\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if face_detector is None:\n",
        "        face_detector = MTCNN(\n",
        "            keep_all=False,\n",
        "            post_process=False,\n",
        "            min_face_size=48,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "    if transform is None:\n",
        "        _, transform = get_default_transforms()\n",
        "\n",
        "    # Get all image files\n",
        "    image_files = []\n",
        "    for f in os.listdir(frames_dir):\n",
        "        if f.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            image_files.append(os.path.join(frames_dir, f))\n",
        "\n",
        "    if not image_files:\n",
        "        print(f\"No image files found in {frames_dir}\")\n",
        "        return []\n",
        "\n",
        "    # Process each frame\n",
        "    results = []\n",
        "    for img_path in image_files:\n",
        "        result = predict_image(model, img_path, transform, device, face_detector)\n",
        "        result['path'] = img_path\n",
        "        results.append(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "def analyze_video_results(results):\n",
        "    \"\"\"Analyze results from multiple frames of a video\"\"\"\n",
        "    if not results:\n",
        "        return {\n",
        "            'prediction': 'Unknown',\n",
        "            'confidence': 0.0,\n",
        "            'error': 'No results provided'\n",
        "        }\n",
        "\n",
        "    # Filter out error results\n",
        "    valid_results = [r for r in results if 'error' not in r or not r['error']]\n",
        "\n",
        "    if not valid_results:\n",
        "        return {\n",
        "            'prediction': 'Unknown',\n",
        "            'confidence': 0.0,\n",
        "            'error': 'No valid frame results'\n",
        "        }\n",
        "\n",
        "    # Count predictions\n",
        "    real_count = sum(1 for r in valid_results if r['prediction'] == 'Real')\n",
        "    fake_count = len(valid_results) - real_count\n",
        "\n",
        "    # Average probabilities\n",
        "    avg_real_prob = sum(r['probabilities'][0] for r in valid_results) / len(valid_results)\n",
        "    avg_fake_prob = sum(r['probabilities'][1] for r in valid_results) / len(valid_results)\n",
        "\n",
        "    # Determine final prediction\n",
        "    if avg_fake_prob > avg_real_prob:\n",
        "        final_pred = 'Fake'\n",
        "        confidence = avg_fake_prob\n",
        "    else:\n",
        "        final_pred = 'Real'\n",
        "        confidence = avg_real_prob\n",
        "\n",
        "    # Create summary\n",
        "    summary = {\n",
        "        'prediction': final_pred,\n",
        "        'confidence': confidence,\n",
        "        'avg_real_prob': avg_real_prob,\n",
        "        'avg_fake_prob': avg_fake_prob,\n",
        "        'real_count': real_count,\n",
        "        'fake_count': fake_count,\n",
        "        'total_frames': len(valid_results),\n",
        "        'detailed_results': valid_results\n",
        "    }\n",
        "\n",
        "    return summary\n",
        "\n",
        "def predict_video(model, video_path, output_dir=None, device=None, num_frames=20):\n",
        "    \"\"\"Predict if a video is real or fake\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Create temporary directory for frames\n",
        "    if output_dir is None:\n",
        "        import tempfile\n",
        "        output_dir = tempfile.mkdtemp()\n",
        "    else:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    frames_dir = os.path.join(output_dir, \"frames\")\n",
        "    os.makedirs(frames_dir, exist_ok=True)\n",
        "\n",
        "    # Extract frames\n",
        "    success = extract_frames_from_video(video_path, frames_dir, num_frames)\n",
        "\n",
        "    if not success:\n",
        "        return {\n",
        "            'prediction': 'Unknown',\n",
        "            'confidence': 0.0,\n",
        "            'error': 'Failed to extract frames'\n",
        "        }\n",
        "\n",
        "    # Initialize face detector\n",
        "    face_detector = MTCNN(\n",
        "        keep_all=False,\n",
        "        post_process=False,\n",
        "        min_face_size=48,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Get transform\n",
        "    _, transform = get_default_transforms()\n",
        "\n",
        "    # Process frames\n",
        "    results = batch_process_frames(model, frames_dir, transform, device, face_detector)\n",
        "\n",
        "    # Analyze results\n",
        "    summary = analyze_video_results(results)\n",
        "    summary['video_path'] = video_path\n",
        "\n",
        "    return summary\n",
        "\n",
        "def visualize_video_results(video_path, results, output_dir=None):\n",
        "    \"\"\"Visualize results from video analysis\"\"\"\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Add title\n",
        "    if 'error' in results and results['error']:\n",
        "        plt.suptitle(f\"Error: {results['error']}\", color='red', fontsize=16)\n",
        "    else:\n",
        "        pred = results['prediction']\n",
        "        conf = results['confidence']\n",
        "        color = 'green' if pred == 'Real' else 'red'\n",
        "        plt.suptitle(f\"Video Prediction: {pred} (Confidence: {conf:.4f})\", color=color, fontsize=16)\n",
        "\n",
        "    # Plot frame-by-frame results if available\n",
        "    if 'detailed_results' in results and results['detailed_results']:\n",
        "        # Get frame indices and fake probabilities\n",
        "        frames = range(len(results['detailed_results']))\n",
        "        fake_probs = [r['probabilities'][1] for r in results['detailed_results']]\n",
        "\n",
        "        plt.plot(frames, fake_probs, 'r-', marker='o', label='Fake Probability')\n",
        "        plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "        plt.ylim(0, 1.05)\n",
        "        plt.xlabel('Frame Index')\n",
        "        plt.ylabel('Fake Probability')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "\n",
        "        # Additional stats\n",
        "        stats = (\n",
        "            f\"Real Frames: {results['real_count']}, \"\n",
        "            f\"Fake Frames: {results['fake_count']}, \"\n",
        "            f\"Total: {results['total_frames']}\\n\"\n",
        "            f\"Avg Real Prob: {results['avg_real_prob']:.4f}, \"\n",
        "            f\"Avg Fake Prob: {results['avg_fake_prob']:.4f}\"\n",
        "        )\n",
        "        plt.title(stats, fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save if output directory is provided\n",
        "    if output_dir:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        video_name = os.path.basename(video_path)\n",
        "        output_file = os.path.join(output_dir, f\"{os.path.splitext(video_name)[0]}_results.png\")\n",
        "        plt.savefig(output_file, dpi=150)\n",
        "        print(f\"Results visualization saved to {output_file}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function for quick tests\"\"\"\n",
        "    # Setup environment\n",
        "    device = setup_environment()\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Example usage\n",
        "    from model import DeiTForDeepfakeDetection\n",
        "\n",
        "    # Create model\n",
        "    model = DeiTForDeepfakeDetection(num_classes=2).to(device)\n",
        "\n",
        "    # Load weights if available\n",
        "    model_path = \"/content/drive/MyDrive/deepfake_detection/checkpoints/best_model.pth\"\n",
        "    if os.path.exists(model_path):\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        print(f\"Loaded model from {model_path}\")\n",
        "    else:\n",
        "        print(f\"Model not found at {model_path}, using initialized model\")\n",
        "\n",
        "    # Example prediction\n",
        "    example_image = \"/content/drive/MyDrive/deepfake_detection/sample_images/test.jpg\"\n",
        "    if os.path.exists(example_image):\n",
        "        result = predict_image(model, example_image, device=device)\n",
        "        visualize_prediction(example_image, result)\n",
        "\n",
        "    # Example video prediction\n",
        "    example_video = \"/content/drive/MyDrive/deepfake_detection/sample_videos/test.mp4\"\n",
        "    if os.path.exists(example_video):\n",
        "        results = predict_video(model, example_video, device=device)\n",
        "        visualize_video_results(example_video, results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bT-tWCGCq5Xd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}